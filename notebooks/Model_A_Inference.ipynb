{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A: Protein Generator Diffusion model conditioned on overall secondary structure content\n",
    "\n",
    "**Citing this work**\n",
    "\n",
    "Any publication that discloses findings arising from using this notebook should cite the following work.\n",
    "\n",
    "B. Ni, D.L. Kaplan, M.J. Buehler, Generative design of de novo proteins based on secondary structure constraints using an attention-based diffusion model, Chem, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "# device\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "# available_gpus\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print('# of GPUs: ', num_of_gpus)\n",
    "\n",
    "# # for debug\n",
    "# device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='Local_Store/output_model_A/'\n",
    "if not os.path.exists(prefix):\n",
    "        print('Create new folder for Model A')\n",
    "        os.mkdir (prefix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ynormfac=22.\n",
    "batch_size_=512\n",
    "max_length = 64\n",
    "number = 99999999999999999\n",
    "min_length=0\n",
    "\n",
    "# define the model\n",
    "embed_dim_position=128\n",
    "pred_dim=1\n",
    "cond_dim = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 22:49:02.289661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 22:49:07.775106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from ProteinDiffusionGenerator.transformer_Model_A import ProteinDesigner_A, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text conditioning is equal to cond_dim - no linear layer used\n",
      "768 1\n",
      "Loss type:  0\n",
      "Channels in=1, channels out=1\n"
     ]
    }
   ],
   "source": [
    "model_A =ProteinDesigner_A(\n",
    "    timesteps=(96), \n",
    "    dim=768, \n",
    "    pred_dim=pred_dim, \n",
    "    loss_type=0, elucidated=True,\n",
    "    padding_idx=0,cond_dim = cond_dim,\n",
    "    text_embed_dim = cond_dim-embed_dim_position,\n",
    "    embed_dim_position=128,\n",
    "    max_text_len=8,\n",
    "    device=device,\n",
    "                )  .to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:  2206419538  trainable parameters:  2206419538\n",
      "Total parameters:  2206417042  trainable parameters:  2206417042\n"
     ]
    }
   ],
   "source": [
    "params (model_A)\n",
    "params (model_A.imagen.unets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#@title ####download the trained models...\n",
    "#@markdown ### If it's the 1st run, this may take tens of minutes\n",
    "\n",
    "# downlowd the trained model if it is available\n",
    "# from Markus: \n",
    "# https://www.dropbox.com/s/wk8sizfbfjaz6yy/Model_B_final.pt?dl=0\n",
    "\n",
    "model_weight_file_final = prefix+'Model_A_final.pt'\n",
    "model_weight_file_early = prefix+'Model_A_early.pt'\n",
    "\n",
    "file_exists = os.path.exists(model_weight_file_final)\n",
    "if not (file_exists):\n",
    "  # download things\n",
    "  print(os.popen(f\"wget https://www.dropbox.com/s/r79cf7uo80z3v15/Model_A_final.pt -P {prefix}\").read())\n",
    "\n",
    "file_exists = os.path.exists(model_weight_file_early)\n",
    "if not (file_exists):\n",
    "  # download things\n",
    "  print(os.popen(f\"wget https://www.dropbox.com/s/mz4afbfs0da4vb2/Model_A_early.pt -P {prefix}\").read())\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "which_checkpoint_to_load = 'final' #@param [\"final\", \"earlystopping\"]\n",
    "\n",
    "if which_checkpoint_to_load == 'final':\n",
    "    fname=f\"{prefix}Model_A_final.pt\"  #Final checkpoint\n",
    "else:\n",
    "    fname=f\"{prefix}Model_A_early.pt\"  #Early stopping checkpoint\n",
    "\n",
    "if device==\"cpu\":\n",
    "    model_A.load_state_dict(torch.load(fname, map_location=torch.device(device=device)))\n",
    "else:\n",
    "    model_A.load_state_dict(torch.load(fname))\n",
    "# model.load_state_dict(torch.load(f\"{path}\", map_location=torch.device(device=device)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-04-23 23:08:05--  https://github.com/Bo-Ni/ProteinDiffusionGenerator_Colab/raw/main/Model_A_tokenizers.dat\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Bo-Ni/ProteinDiffusionGenerator_Colab/main/Model_A_tokenizers.dat [following]\n",
      "--2023-04-23 23:08:06--  https://raw.githubusercontent.com/Bo-Ni/ProteinDiffusionGenerator_Colab/main/Model_A_tokenizers.dat\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 861 [application/octet-stream]\n",
      "Saving to: ‘Model_A_tokenizers.dat’\n",
      "\n",
      "     0K                                                       100% 12.6M=0s\n",
      "\n",
      "2023-04-23 23:08:06 (12.6 MB/s) - ‘Model_A_tokenizers.dat’ saved [861/861]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 23:08:11.067093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 23:08:16.592220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# load in the tokenizer\n",
    "import pickle\n",
    "tokenizer_file = prefix+'Model_A_tokenizers.dat'\n",
    "\n",
    "# https://github.com/Bo-Ni/ProteinDiffusionGenerator_Colab/raw/main/Model_A_tokenizers.dat\n",
    "file_exists = os.path.exists(tokenizer_file)\n",
    "if not (file_exists):\n",
    "  # download things\n",
    "  print(os.popen(F\"wget https://github.com/Bo-Ni/ProteinDiffusionGenerator_Colab/raw/main/Model_A_tokenizers.dat -P {prefix}\").read())\n",
    "with open(tokenizer_file, \"rb\") as f:\n",
    " tokenizer_data_1 = pickle.load(f)\n",
    "  # print(pickle.load(f))\n",
    "# print(tokenizer_data_1[0])\n",
    "tokenizer_y = tokenizer_data_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProteinDiffusionGenerator.transformer_Model_A import sample_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ####Example 2: input vectors\n",
    "\n",
    "#### Generate candidates\n",
    "\n",
    "vec_1 = '[0, 0.7, 0.07, 0.1, 0.01, 0.02, 0.01, 0.11]' \n",
    "\n",
    "\n",
    "in_vec_1 = np.array(np.matrix(vec_1)).ravel()\n",
    "\n",
    "norm_flag = False\n",
    "flag_ref=40000\n",
    "\n",
    "print('Working on vec_1:')\n",
    "sample_sequence (model_A,\n",
    "                X=[in_vec_1], \n",
    "                    normalize_input_to_one=norm_flag,\n",
    "                 flag=flag_ref,cond_scales=1.,foldproteins=True,calc_error=True,\n",
    "                 # +++++++++++++++++++++++++++++++++++++++=\n",
    "                 prefix=prefix,\n",
    "                 ynormfac=ynormfac,\n",
    "                 tokenizer_y=tokenizer_y,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinDesigner_A(\n",
       "  (fc_embed1): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (fc_embed2): Linear(in_features=1, out_features=384, bias=True)\n",
       "  (pos_emb_x): Embedding(9, 128)\n",
       "  (imagen): ElucidatedImagen(\n",
       "    (lowres_noise_schedule): GaussianDiffusionContinuousTimes()\n",
       "    (unets): ModuleList(\n",
       "      (0): OneD_Unet(\n",
       "        (init_conv): Conv1d(1, 768, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (to_time_hiddens): Sequential(\n",
       "          (0): LearnedSinusoidalPosEmb()\n",
       "          (1): Linear(in_features=17, out_features=3072, bias=True)\n",
       "          (2): SiLU()\n",
       "        )\n",
       "        (to_time_cond): Sequential(\n",
       "          (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        )\n",
       "        (to_time_tokens): Sequential(\n",
       "          (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "          (1): Rearrange('b (r d) -> b r d', r=2)\n",
       "        )\n",
       "        (norm_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn_pool): PerceiverResampler(\n",
       "          (pos_emb): Embedding(512, 512)\n",
       "          (to_latents_from_mean_pooled_seq): Sequential(\n",
       "            (0): LayerNorm()\n",
       "            (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (2): Rearrange('b (n d) -> b n d', n=4)\n",
       "          )\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PerceiverAttention(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm_latents): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (2): GELU(approximate=none)\n",
       "                (3): LayerNorm()\n",
       "                (4): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): PerceiverAttention(\n",
       "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm_latents): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): LayerNorm()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (2): GELU(approximate=none)\n",
       "                (3): LayerNorm()\n",
       "                (4): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (to_text_non_attn_cond): Sequential(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=512, out_features=3072, bias=True)\n",
       "          (2): SiLU()\n",
       "          (3): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        )\n",
       "        (downs): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): None\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (time_mlp): Sequential(\n",
       "                  (0): SiLU()\n",
       "                  (1): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                )\n",
       "                (block1): Block(\n",
       "                  (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (block2): Block(\n",
       "                  (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (gca): GlobalContext(\n",
       "                  (to_k): Conv1d(768, 1, kernel_size=(1,), stride=(1,))\n",
       "                  (net): Sequential(\n",
       "                    (0): Conv1d(768, 384, kernel_size=(1,), stride=(1,))\n",
       "                    (1): SiLU()\n",
       "                    (2): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
       "                    (3): Sigmoid()\n",
       "                  )\n",
       "                )\n",
       "                (res_conv): Identity()\n",
       "              )\n",
       "            )\n",
       "            (3): Identity()\n",
       "            (4): Sequential(\n",
       "              (0): Rearrange('b c (h s1)  -> b (c s1) h', s1=2)\n",
       "              (1): Conv1d(1536, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): None\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=768, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (time_mlp): Sequential(\n",
       "                  (0): SiLU()\n",
       "                  (1): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                )\n",
       "                (block1): Block(\n",
       "                  (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (block2): Block(\n",
       "                  (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (gca): GlobalContext(\n",
       "                  (to_k): Conv1d(768, 1, kernel_size=(1,), stride=(1,))\n",
       "                  (net): Sequential(\n",
       "                    (0): Conv1d(768, 384, kernel_size=(1,), stride=(1,))\n",
       "                    (1): SiLU()\n",
       "                    (2): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
       "                    (3): Sigmoid()\n",
       "                  )\n",
       "                )\n",
       "                (res_conv): Identity()\n",
       "              )\n",
       "            )\n",
       "            (3): TransformerBlock(\n",
       "              (layers): ModuleList(\n",
       "                (0): ModuleList(\n",
       "                  (0): EinopsToAndFrom(\n",
       "                    (fn): Attention(\n",
       "                      (norm): LayerNorm()\n",
       "                      (to_q): Linear(in_features=768, out_features=512, bias=False)\n",
       "                      (to_kv): Linear(in_features=768, out_features=128, bias=False)\n",
       "                      (to_context): Sequential(\n",
       "                        (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "                      )\n",
       "                      (to_out): Sequential(\n",
       "                        (0): Linear(in_features=512, out_features=768, bias=False)\n",
       "                        (1): LayerNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): LayerNorm()\n",
       "                    (1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                    (2): GELU(approximate=none)\n",
       "                    (3): LayerNorm()\n",
       "                    (4): Conv1d(1536, 768, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): Sequential(\n",
       "              (0): Rearrange('b c (h s1)  -> b (c s1) h', s1=2)\n",
       "              (1): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): None\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1536, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (time_mlp): Sequential(\n",
       "                  (0): SiLU()\n",
       "                  (1): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "                )\n",
       "                (block1): Block(\n",
       "                  (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (block2): Block(\n",
       "                  (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (gca): GlobalContext(\n",
       "                  (to_k): Conv1d(1536, 1, kernel_size=(1,), stride=(1,))\n",
       "                  (net): Sequential(\n",
       "                    (0): Conv1d(1536, 768, kernel_size=(1,), stride=(1,))\n",
       "                    (1): SiLU()\n",
       "                    (2): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
       "                    (3): Sigmoid()\n",
       "                  )\n",
       "                )\n",
       "                (res_conv): Identity()\n",
       "              )\n",
       "            )\n",
       "            (3): TransformerBlock(\n",
       "              (layers): ModuleList(\n",
       "                (0): ModuleList(\n",
       "                  (0): EinopsToAndFrom(\n",
       "                    (fn): Attention(\n",
       "                      (norm): LayerNorm()\n",
       "                      (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                      (to_kv): Linear(in_features=1536, out_features=128, bias=False)\n",
       "                      (to_context): Sequential(\n",
       "                        (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "                      )\n",
       "                      (to_out): Sequential(\n",
       "                        (0): Linear(in_features=512, out_features=1536, bias=False)\n",
       "                        (1): LayerNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): LayerNorm()\n",
       "                    (1): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                    (2): GELU(approximate=none)\n",
       "                    (3): LayerNorm()\n",
       "                    (4): Conv1d(3072, 1536, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): Sequential(\n",
       "              (0): Rearrange('b c (h s1)  -> b (c s1) h', s1=2)\n",
       "              (1): Conv1d(3072, 3072, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): None\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=3072, out_features=6144, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (groupnorm): GroupNorm(8, 3072, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(3072, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (groupnorm): GroupNorm(8, 3072, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(3072, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (time_mlp): Sequential(\n",
       "                  (0): SiLU()\n",
       "                  (1): Linear(in_features=3072, out_features=6144, bias=True)\n",
       "                )\n",
       "                (block1): Block(\n",
       "                  (groupnorm): GroupNorm(8, 3072, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(3072, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (block2): Block(\n",
       "                  (groupnorm): GroupNorm(8, 3072, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(3072, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (gca): GlobalContext(\n",
       "                  (to_k): Conv1d(3072, 1, kernel_size=(1,), stride=(1,))\n",
       "                  (net): Sequential(\n",
       "                    (0): Conv1d(3072, 1536, kernel_size=(1,), stride=(1,))\n",
       "                    (1): SiLU()\n",
       "                    (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "                    (3): Sigmoid()\n",
       "                  )\n",
       "                )\n",
       "                (res_conv): Identity()\n",
       "              )\n",
       "            )\n",
       "            (3): Identity()\n",
       "            (4): Parallel(\n",
       "              (fns): ModuleList(\n",
       "                (0): Conv1d(3072, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                (1): Conv1d(3072, 6144, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ups): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (groupnorm): GroupNorm(8, 9216, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(9216, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (groupnorm): GroupNorm(8, 6144, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(6144, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (res_conv): Conv1d(9216, 6144, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (time_mlp): Sequential(\n",
       "                  (0): SiLU()\n",
       "                  (1): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "                )\n",
       "                (block1): Block(\n",
       "                  (groupnorm): GroupNorm(8, 9216, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(9216, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (block2): Block(\n",
       "                  (groupnorm): GroupNorm(8, 6144, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(6144, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (gca): GlobalContext(\n",
       "                  (to_k): Conv1d(6144, 1, kernel_size=(1,), stride=(1,))\n",
       "                  (net): Sequential(\n",
       "                    (0): Conv1d(6144, 3072, kernel_size=(1,), stride=(1,))\n",
       "                    (1): SiLU()\n",
       "                    (2): Conv1d(3072, 6144, kernel_size=(1,), stride=(1,))\n",
       "                    (3): Sigmoid()\n",
       "                  )\n",
       "                )\n",
       "                (res_conv): Conv1d(9216, 6144, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): Identity()\n",
       "            (3): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "              (1): Conv1d(6144, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=3072, out_features=6144, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (to_q): Linear(in_features=3072, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=3072, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (groupnorm): GroupNorm(8, 4608, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(4608, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (groupnorm): GroupNorm(8, 3072, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(3072, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (res_conv): Conv1d(4608, 3072, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (time_mlp): Sequential(\n",
       "                  (0): SiLU()\n",
       "                  (1): Linear(in_features=3072, out_features=6144, bias=True)\n",
       "                )\n",
       "                (block1): Block(\n",
       "                  (groupnorm): GroupNorm(8, 4608, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(4608, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (block2): Block(\n",
       "                  (groupnorm): GroupNorm(8, 3072, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(3072, 3072, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (gca): GlobalContext(\n",
       "                  (to_k): Conv1d(3072, 1, kernel_size=(1,), stride=(1,))\n",
       "                  (net): Sequential(\n",
       "                    (0): Conv1d(3072, 1536, kernel_size=(1,), stride=(1,))\n",
       "                    (1): SiLU()\n",
       "                    (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "                    (3): Sigmoid()\n",
       "                  )\n",
       "                )\n",
       "                (res_conv): Conv1d(4608, 3072, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): TransformerBlock(\n",
       "              (layers): ModuleList(\n",
       "                (0): ModuleList(\n",
       "                  (0): EinopsToAndFrom(\n",
       "                    (fn): Attention(\n",
       "                      (norm): LayerNorm()\n",
       "                      (to_q): Linear(in_features=3072, out_features=512, bias=False)\n",
       "                      (to_kv): Linear(in_features=3072, out_features=128, bias=False)\n",
       "                      (to_context): Sequential(\n",
       "                        (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "                      )\n",
       "                      (to_out): Sequential(\n",
       "                        (0): Linear(in_features=512, out_features=3072, bias=False)\n",
       "                        (1): LayerNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): LayerNorm()\n",
       "                    (1): Conv1d(3072, 6144, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                    (2): GELU(approximate=none)\n",
       "                    (3): LayerNorm()\n",
       "                    (4): Conv1d(6144, 3072, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "              (1): Conv1d(3072, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1536, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (groupnorm): GroupNorm(8, 2304, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(2304, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (res_conv): Conv1d(2304, 1536, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (time_mlp): Sequential(\n",
       "                  (0): SiLU()\n",
       "                  (1): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "                )\n",
       "                (block1): Block(\n",
       "                  (groupnorm): GroupNorm(8, 2304, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(2304, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (block2): Block(\n",
       "                  (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (gca): GlobalContext(\n",
       "                  (to_k): Conv1d(1536, 1, kernel_size=(1,), stride=(1,))\n",
       "                  (net): Sequential(\n",
       "                    (0): Conv1d(1536, 768, kernel_size=(1,), stride=(1,))\n",
       "                    (1): SiLU()\n",
       "                    (2): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
       "                    (3): Sigmoid()\n",
       "                  )\n",
       "                )\n",
       "                (res_conv): Conv1d(2304, 1536, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): TransformerBlock(\n",
       "              (layers): ModuleList(\n",
       "                (0): ModuleList(\n",
       "                  (0): EinopsToAndFrom(\n",
       "                    (fn): Attention(\n",
       "                      (norm): LayerNorm()\n",
       "                      (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                      (to_kv): Linear(in_features=1536, out_features=128, bias=False)\n",
       "                      (to_context): Sequential(\n",
       "                        (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                        (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "                      )\n",
       "                      (to_out): Sequential(\n",
       "                        (0): Linear(in_features=512, out_features=1536, bias=False)\n",
       "                        (1): LayerNorm()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): LayerNorm()\n",
       "                    (1): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                    (2): GELU(approximate=none)\n",
       "                    (3): LayerNorm()\n",
       "                    (4): Conv1d(3072, 1536, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "              (1): Conv1d(1536, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(1536, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                (activation): SiLU()\n",
       "                (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              )\n",
       "              (res_conv): Conv1d(1536, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (time_mlp): Sequential(\n",
       "                  (0): SiLU()\n",
       "                  (1): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                )\n",
       "                (block1): Block(\n",
       "                  (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(1536, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (block2): Block(\n",
       "                  (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "                  (activation): SiLU()\n",
       "                  (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "                )\n",
       "                (gca): GlobalContext(\n",
       "                  (to_k): Conv1d(768, 1, kernel_size=(1,), stride=(1,))\n",
       "                  (net): Sequential(\n",
       "                    (0): Conv1d(768, 384, kernel_size=(1,), stride=(1,))\n",
       "                    (1): SiLU()\n",
       "                    (2): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
       "                    (3): Sigmoid()\n",
       "                  )\n",
       "                )\n",
       "                (res_conv): Conv1d(1536, 768, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): Identity()\n",
       "            (3): Identity()\n",
       "          )\n",
       "        )\n",
       "        (mid_block1): ResnetBlock(\n",
       "          (time_mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "          )\n",
       "          (cross_attn): EinopsToAndFrom(\n",
       "            (fn): CrossAttention(\n",
       "              (norm): LayerNorm()\n",
       "              (norm_context): Identity()\n",
       "              (to_q): Linear(in_features=6144, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=6144, bias=False)\n",
       "                (1): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block1): Block(\n",
       "            (groupnorm): GroupNorm(8, 6144, eps=1e-05, affine=True)\n",
       "            (activation): SiLU()\n",
       "            (project): Conv1d(6144, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (block2): Block(\n",
       "            (groupnorm): GroupNorm(8, 6144, eps=1e-05, affine=True)\n",
       "            (activation): SiLU()\n",
       "            (project): Conv1d(6144, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (res_conv): Identity()\n",
       "        )\n",
       "        (mid_attn): EinopsToAndFrom(\n",
       "          (fn): Residual(\n",
       "            (fn): Attention(\n",
       "              (norm): LayerNorm()\n",
       "              (to_q): Linear(in_features=6144, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=6144, out_features=128, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=6144, bias=False)\n",
       "                (1): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mid_block2): ResnetBlock(\n",
       "          (time_mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "          )\n",
       "          (cross_attn): EinopsToAndFrom(\n",
       "            (fn): CrossAttention(\n",
       "              (norm): LayerNorm()\n",
       "              (norm_context): Identity()\n",
       "              (to_q): Linear(in_features=6144, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=6144, bias=False)\n",
       "                (1): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (block1): Block(\n",
       "            (groupnorm): GroupNorm(8, 6144, eps=1e-05, affine=True)\n",
       "            (activation): SiLU()\n",
       "            (project): Conv1d(6144, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (block2): Block(\n",
       "            (groupnorm): GroupNorm(8, 6144, eps=1e-05, affine=True)\n",
       "            (activation): SiLU()\n",
       "            (project): Conv1d(6144, 6144, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (res_conv): Identity()\n",
       "        )\n",
       "        (upsample_combiner): UpsampleCombiner(\n",
       "          (fmap_convs): ModuleList(\n",
       "            (0): Block(\n",
       "              (groupnorm): GroupNorm(8, 6144, eps=1e-05, affine=True)\n",
       "              (activation): SiLU()\n",
       "              (project): Conv1d(6144, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (1): Block(\n",
       "              (groupnorm): GroupNorm(8, 3072, eps=1e-05, affine=True)\n",
       "              (activation): SiLU()\n",
       "              (project): Conv1d(3072, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (2): Block(\n",
       "              (groupnorm): GroupNorm(8, 1536, eps=1e-05, affine=True)\n",
       "              (activation): SiLU()\n",
       "              (project): Conv1d(1536, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "            (3): Block(\n",
       "              (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "              (activation): SiLU()\n",
       "              (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_res_block): ResnetBlock(\n",
       "          (time_mlp): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          )\n",
       "          (block1): Block(\n",
       "            (groupnorm): GroupNorm(8, 3840, eps=1e-05, affine=True)\n",
       "            (activation): SiLU()\n",
       "            (project): Conv1d(3840, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (block2): Block(\n",
       "            (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "            (activation): SiLU()\n",
       "            (project): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          )\n",
       "          (gca): GlobalContext(\n",
       "            (to_k): Conv1d(768, 1, kernel_size=(1,), stride=(1,))\n",
       "            (net): Sequential(\n",
       "              (0): Conv1d(768, 384, kernel_size=(1,), stride=(1,))\n",
       "              (1): SiLU()\n",
       "              (2): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
       "              (3): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (res_conv): Conv1d(3840, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (final_conv): Conv1d(768, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_A.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
